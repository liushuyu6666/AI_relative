{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"autograd_tutorial.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"HP0W4R8WnpXZ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600658190635,"user_tz":240,"elapsed":732,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}}},"source":["%matplotlib inline"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rXPybUMRnpXf","colab_type":"text"},"source":["\n","Autograd: Automatic Differentiation\n","===================================\n","\n","Central to all neural networks in PyTorch is the ``autograd`` package.\n","Let’s first briefly visit this, and we will then go to training our\n","first neural network.\n","\n","\n","The ``autograd`` package provides automatic differentiation for all operations\n","on Tensors. It is a define-by-run framework, which means that your backprop is\n","defined by how your code is run, and that every single iteration can be\n","different.\n","\n","Let us see this in more simple terms with some examples.\n","\n","Tensor\n","--------\n","\n","``torch.Tensor`` is the central class of the package. If you set its attribute\n","``.requires_grad`` as ``True``, it starts to track all operations on it. When\n","you finish your computation you can call ``.backward()`` and have all the\n","gradients computed automatically. The gradient for this tensor will be\n","accumulated into ``.grad`` attribute.\n","\n","To stop a tensor from tracking history, you can call ``.detach()`` to detach\n","it from the computation history, and to prevent future computation from being\n","tracked.\n","\n","To prevent tracking history (and using memory), you can also wrap the code block\n","in ``with torch.no_grad():``. This can be particularly helpful when evaluating a\n","model because the model may have trainable parameters with\n","``requires_grad=True``, but for which we don't need the gradients.\n","\n","There’s one more class which is very important for autograd\n","implementation - a ``Function``.\n","\n","``Tensor`` and ``Function`` are interconnected and build up an acyclic\n","graph, that encodes a complete history of computation. Each tensor has\n","a ``.grad_fn`` attribute that references a ``Function`` that has created\n","the ``Tensor`` (except for Tensors created by the user - their\n","``grad_fn is None``).\n","\n","If you want to compute the derivatives, you can call ``.backward()`` on\n","a ``Tensor``. If ``Tensor`` is a scalar (i.e. it holds a one element\n","data), you don’t need to specify any arguments to ``backward()``,\n","however if it has more elements, you need to specify a ``gradient``\n","argument that is a tensor of matching shape.\n","\n"]},{"cell_type":"code","metadata":{"id":"0RZWrFROnpXf","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600658193572,"user_tz":240,"elapsed":3665,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}}},"source":["import torch"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"D5JhLvh9hQUH","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600658195159,"user_tz":240,"elapsed":548,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}}},"source":["# print all attributes\n","def DCG_node(listValue):\n","    for (i, v) in enumerate(listValue):\n","        print(f\"{i}.data: {v.data}\\n{i}.requires_grad: {v.requires_grad}\\n{i}.grad: {v.grad}\\n{i}.grad_fn: {v.grad_fn}\\n{i}.is_leaf: {v.is_leaf}\\n\")\n"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8T13vVJdC9h1","colab_type":"text"},"source":["Generate a calculation tree\n","------------------------\n","Create a tensor and set ``requires_grad=True`` to track computation with it"]},{"cell_type":"code","metadata":{"id":"vyizfip6npXi","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600658198753,"user_tz":240,"elapsed":1062,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}}},"source":["x = torch.ones(2, 2, requires_grad=True)\n","y = x + 2\n","z = y * y * 3\n","out = z.mean()\n","l = out * 2"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oasvbrAanw34","colab_type":"text"},"source":["check the Dynamic Computational graph of each node"]},{"cell_type":"markdown","metadata":{"id":"tpws-LFzCvDX","colab_type":"text"},"source":["Here we can see x as an initial point of this calculation series, marked as leaf"]},{"cell_type":"code","metadata":{"id":"XN6FhRsLotuv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":625},"executionInfo":{"status":"ok","timestamp":1600658202945,"user_tz":240,"elapsed":489,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}},"outputId":"fd8dead3-80d5-40b2-bb35-0869044b91d4"},"source":["DCG_node([x, y, z, out, l])"],"execution_count":5,"outputs":[{"output_type":"stream","text":["0.data: tensor([[1., 1.],\n","        [1., 1.]])\n","0.requires_grad: True\n","0.grad: None\n","0.grad_fn: None\n","0.is_leaf: True\n","\n","1.data: tensor([[3., 3.],\n","        [3., 3.]])\n","1.requires_grad: True\n","1.grad: None\n","1.grad_fn: <AddBackward0 object at 0x7fc47c767c18>\n","1.is_leaf: False\n","\n","2.data: tensor([[27., 27.],\n","        [27., 27.]])\n","2.requires_grad: True\n","2.grad: None\n","2.grad_fn: <MulBackward0 object at 0x7fc47c767be0>\n","2.is_leaf: False\n","\n","3.data: 27.0\n","3.requires_grad: True\n","3.grad: None\n","3.grad_fn: <MeanBackward0 object at 0x7fc47c767ba8>\n","3.is_leaf: False\n","\n","4.data: 54.0\n","4.requires_grad: True\n","4.grad: None\n","4.grad_fn: <MulBackward0 object at 0x7fc47c767ba8>\n","4.is_leaf: False\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  after removing the cwd from sys.path.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"o1Wr0B81npXv","colab_type":"text"},"source":["``.requires_grad_( ... )`` changes an existing Tensor's ``requires_grad``\n","flag in-place. The input flag defaults to ``False`` if not given.\n","\n"]},{"cell_type":"code","metadata":{"id":"yDufAeE0npXv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":625},"executionInfo":{"status":"ok","timestamp":1600658207081,"user_tz":240,"elapsed":612,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}},"outputId":"aad84934-c74e-49c0-af0c-ccaf03f841fd"},"source":["x.requires_grad_(True) # you can't specify y.requires_grad_(False)\n","DCG_node([x, y, z, out, l])"],"execution_count":6,"outputs":[{"output_type":"stream","text":["0.data: tensor([[1., 1.],\n","        [1., 1.]])\n","0.requires_grad: True\n","0.grad: None\n","0.grad_fn: None\n","0.is_leaf: True\n","\n","1.data: tensor([[3., 3.],\n","        [3., 3.]])\n","1.requires_grad: True\n","1.grad: None\n","1.grad_fn: <AddBackward0 object at 0x7fc47c767cf8>\n","1.is_leaf: False\n","\n","2.data: tensor([[27., 27.],\n","        [27., 27.]])\n","2.requires_grad: True\n","2.grad: None\n","2.grad_fn: <MulBackward0 object at 0x7fc47c767c88>\n","2.is_leaf: False\n","\n","3.data: 27.0\n","3.requires_grad: True\n","3.grad: None\n","3.grad_fn: <MeanBackward0 object at 0x7fc47c767d68>\n","3.is_leaf: False\n","\n","4.data: 54.0\n","4.requires_grad: True\n","4.grad: None\n","4.grad_fn: <MulBackward0 object at 0x7fc47c767d68>\n","4.is_leaf: False\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  after removing the cwd from sys.path.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"ovksRp97npXy","colab_type":"text"},"source":["Gradients\n","---------\n","Let's backprop now.\n","Because ``out`` contains a single scalar, ``out.backward()`` is\n","equivalent to ``out.backward(torch.tensor(L.))``.\n","\n"]},{"cell_type":"code","metadata":{"id":"TjfLfGvFnpXy","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600658211754,"user_tz":240,"elapsed":501,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}}},"source":["out.backward(retain_graph=True) # δL / δout = 1: L = out + constant"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f9V53yX2p4a2","colab_type":"text"},"source":["The grad of each node was calculated"]},{"cell_type":"code","metadata":{"id":"Lc2caamDp1IX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":642},"executionInfo":{"status":"ok","timestamp":1600658233300,"user_tz":240,"elapsed":487,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}},"outputId":"add6dc7e-cde6-4ffc-93d0-2607516b7ce2"},"source":["DCG_node([x, y, z, out, l])"],"execution_count":8,"outputs":[{"output_type":"stream","text":["0.data: tensor([[1., 1.],\n","        [1., 1.]])\n","0.requires_grad: True\n","0.grad: tensor([[4.5000, 4.5000],\n","        [4.5000, 4.5000]])\n","0.grad_fn: None\n","0.is_leaf: True\n","\n","1.data: tensor([[3., 3.],\n","        [3., 3.]])\n","1.requires_grad: True\n","1.grad: None\n","1.grad_fn: <AddBackward0 object at 0x7fc47c767d30>\n","1.is_leaf: False\n","\n","2.data: tensor([[27., 27.],\n","        [27., 27.]])\n","2.requires_grad: True\n","2.grad: None\n","2.grad_fn: <MulBackward0 object at 0x7fc47c767c88>\n","2.is_leaf: False\n","\n","3.data: 27.0\n","3.requires_grad: True\n","3.grad: None\n","3.grad_fn: <MeanBackward0 object at 0x7fc47c767cf8>\n","3.is_leaf: False\n","\n","4.data: 54.0\n","4.requires_grad: True\n","4.grad: None\n","4.grad_fn: <MulBackward0 object at 0x7fc47c767cf8>\n","4.is_leaf: False\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  after removing the cwd from sys.path.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"j4HOkghQpRzg","colab_type":"text"},"source":["let's try l.backward()"]},{"cell_type":"code","metadata":{"id":"KAtWUVyKpWL_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":642},"executionInfo":{"status":"ok","timestamp":1600658236827,"user_tz":240,"elapsed":504,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}},"outputId":"d5c57810-428e-41d1-deac-13dcc37a4b0a"},"source":["l.backward(retain_graph=True)\n","DCG_node([x, y, z, out, l])"],"execution_count":9,"outputs":[{"output_type":"stream","text":["0.data: tensor([[1., 1.],\n","        [1., 1.]])\n","0.requires_grad: True\n","0.grad: tensor([[13.5000, 13.5000],\n","        [13.5000, 13.5000]])\n","0.grad_fn: None\n","0.is_leaf: True\n","\n","1.data: tensor([[3., 3.],\n","        [3., 3.]])\n","1.requires_grad: True\n","1.grad: None\n","1.grad_fn: <AddBackward0 object at 0x7fc4c6eac400>\n","1.is_leaf: False\n","\n","2.data: tensor([[27., 27.],\n","        [27., 27.]])\n","2.requires_grad: True\n","2.grad: None\n","2.grad_fn: <MulBackward0 object at 0x7fc4c6eac470>\n","2.is_leaf: False\n","\n","3.data: 27.0\n","3.requires_grad: True\n","3.grad: None\n","3.grad_fn: <MeanBackward0 object at 0x7fc4c6eac358>\n","3.is_leaf: False\n","\n","4.data: 54.0\n","4.requires_grad: True\n","4.grad: None\n","4.grad_fn: <MulBackward0 object at 0x7fc4c6eac358>\n","4.is_leaf: False\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  after removing the cwd from sys.path.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"0hEdBoYR5BGd","colab_type":"text"},"source":["the expected result of x.grad is 9 but actually, and unfortunately, x.grad is 9 + 4.5(the result when we calculate out.backward()). so we need to set the grad.data to zero"]},{"cell_type":"code","metadata":{"id":"FqmdGVcB53Es","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":642},"executionInfo":{"status":"ok","timestamp":1600488559143,"user_tz":240,"elapsed":760,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}},"outputId":"03fb74fa-3c3c-4851-f2b9-df77ac60b94a"},"source":["x.grad.data.zero_()\n","l.backward(retain_graph=True)\n","DCG_node([x, y, z, out, l])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.data: tensor([[1., 1.],\n","        [1., 1.]])\n","0.requires_grad: True\n","0.grad: tensor([[9., 9.],\n","        [9., 9.]])\n","0.grad_fn: None\n","0.is_leaf: True\n","\n","1.data: tensor([[3., 3.],\n","        [3., 3.]])\n","1.requires_grad: True\n","1.grad: None\n","1.grad_fn: <AddBackward0 object at 0x7f32c41d8160>\n","1.is_leaf: False\n","\n","2.data: tensor([[27., 27.],\n","        [27., 27.]])\n","2.requires_grad: True\n","2.grad: None\n","2.grad_fn: <MulBackward0 object at 0x7f32c41d8c88>\n","2.is_leaf: False\n","\n","3.data: 27.0\n","3.requires_grad: True\n","3.grad: None\n","3.grad_fn: <MeanBackward0 object at 0x7f32c41d8a58>\n","3.is_leaf: False\n","\n","4.data: 54.0\n","4.requires_grad: True\n","4.grad: None\n","4.grad_fn: <MulBackward0 object at 0x7f32c41d8a58>\n","4.is_leaf: False\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  after removing the cwd from sys.path.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"vf5Jft49qVx4","colab_type":"text"},"source":["what if we use y.backward()? error raise: grad can be implicitly created only for scalar outputs"]},{"cell_type":"code","metadata":{"id":"Ld9xMiSsqgFv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":370},"executionInfo":{"status":"error","timestamp":1600488576510,"user_tz":240,"elapsed":736,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}},"outputId":"963e55eb-20f5-4941-ba57-697a4e56e1be"},"source":["y.backward()\n","DCG_node([x, y, z, out, l])"],"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-33-434f30bffecc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mDCG_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"]}]},{"cell_type":"markdown","metadata":{"id":"w1h6ZkbwnpX3","colab_type":"text"},"source":["You should have got a matrix of ``4.5``. Let’s call the ``out``\n","*Tensor* “$o$”.\n","We have that $o = \\frac{1}{4}\\sum_i z_i$,\n","$z_i = 3(x_i+2)^2$ and $z_i\\bigr\\rvert_{x_i=1} = 27$.\n","Therefore,\n","$\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)$, hence\n","$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rQbe0rp7npX3","colab_type":"text"},"source":["Mathematically, if you have a vector valued function $\\vec{y}=f(\\vec{x})$,\n","then the gradient of $\\vec{y}$ with respect to $\\vec{x}$\n","is a Jacobian matrix:\n","\n","\\begin{align}J=\\left(\\begin{array}{ccc}\n","   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n","   \\vdots & \\ddots & \\vdots\\\\\n","   \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n","   \\end{array}\\right)\\end{align}\n","\n","Generally speaking, ``torch.autograd`` is an engine for computing\n","vector-Jacobian product. That is, given any vector\n","$v=\\left(\\begin{array}{cccc} v_{1} & v_{2} & \\cdots & v_{m}\\end{array}\\right)^{T}$,\n","compute the product $v^{T}\\cdot J$. If $v$ happens to be\n","the gradient of a scalar function $l=g\\left(\\vec{y}\\right)$,\n","that is,\n","$v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}$,\n","then by the chain rule, the vector-Jacobian product would be the\n","gradient of $l$ with respect to $\\vec{x}$:\n","\n","\\begin{align}J^{T}\\cdot v=\\left(\\begin{array}{ccc}\n","   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n","   \\vdots & \\ddots & \\vdots\\\\\n","   \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n","   \\end{array}\\right)\\left(\\begin{array}{c}\n","   \\frac{\\partial l}{\\partial y_{1}}\\\\\n","   \\vdots\\\\\n","   \\frac{\\partial l}{\\partial y_{m}}\n","   \\end{array}\\right)=\\left(\\begin{array}{c}\n","   \\frac{\\partial l}{\\partial x_{1}}\\\\\n","   \\vdots\\\\\n","   \\frac{\\partial l}{\\partial x_{n}}\n","   \\end{array}\\right)\\end{align}\n","\n","(Note that $v^{T}\\cdot J$ gives a row vector which can be\n","treated as a column vector by taking $J^{T}\\cdot v$.)\n","\n","This characteristic of vector-Jacobian product makes it very\n","convenient to feed external gradients into a model that has\n","non-scalar output.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6q_08BxHnpX3","colab_type":"text"},"source":["Now let's take a look at an example of vector-Jacobian product:\n","\n"]},{"cell_type":"code","metadata":{"id":"wnZXQRAinpX4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1600488583348,"user_tz":240,"elapsed":418,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}},"outputId":"2356f7ca-97d7-435d-dd1b-456d4ea24ec9"},"source":["x = torch.randn(3, requires_grad=True)\n","print(x)\n","\n","y = x * 2\n","while y.data.norm() < 1000:\n","    y = y * 2\n","\n","print(y/x)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([-2.0547,  0.4526,  0.5030], requires_grad=True)\n","tensor([512., 512., 512.], grad_fn=<DivBackward0>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UijRP-r2uGAp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":272},"executionInfo":{"status":"ok","timestamp":1600488634503,"user_tz":240,"elapsed":409,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}},"outputId":"b551fc4c-2a7c-4e71-fce2-11b065f3019b"},"source":["DCG_node([x, y])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.data: tensor([-2.0547,  0.4526,  0.5030])\n","0.requires_grad: True\n","0.grad: None\n","0.grad_fn: None\n","0.is_leaf: True\n","\n","1.data: tensor([-1052.0129,   231.7180,   257.5281])\n","1.requires_grad: True\n","1.grad: None\n","1.grad_fn: <MulBackward0 object at 0x7f32bbd510f0>\n","1.is_leaf: False\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  after removing the cwd from sys.path.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"5ud2wu9OnpX6","colab_type":"text"},"source":["Now in this case ``y`` is no longer a scalar. ``torch.autograd``\n","could not compute the full Jacobian directly, but if we just\n","want the vector-Jacobian product, simply pass the vector to\n","``backward`` as argument:\n","\n"]},{"cell_type":"code","metadata":{"id":"XFyg94sPnpX7","colab_type":"code","colab":{}},"source":["v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n","# v = dl / dy, so y.backward(v) mean the supreme is l, and grad has been calculated and populated\n","# pay attention, l is a scalar value\n","# v1 = dl / dy1, v2 = dl / dy2, v3 = dl / dy3\n","# l = y1 * v1 + y2 * v2 + y3 * v3\n","y.backward(v)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XQIZDPwRuT_l","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"status":"ok","timestamp":1600488652412,"user_tz":240,"elapsed":456,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}},"outputId":"5b02f43b-1433-4dbf-c8bd-217919d949bf"},"source":["DCG_node([x, y, v])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.data: tensor([-2.0547,  0.4526,  0.5030])\n","0.requires_grad: True\n","0.grad: tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])\n","0.grad_fn: None\n","0.is_leaf: True\n","\n","1.data: tensor([-1052.0129,   231.7180,   257.5281])\n","1.requires_grad: True\n","1.grad: None\n","1.grad_fn: <MulBackward0 object at 0x7f32bbd51630>\n","1.is_leaf: False\n","\n","2.data: tensor([1.0000e-01, 1.0000e+00, 1.0000e-04])\n","2.requires_grad: False\n","2.grad: None\n","2.grad_fn: None\n","2.is_leaf: True\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  after removing the cwd from sys.path.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Dm3rsESCuyG2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600488659477,"user_tz":240,"elapsed":446,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}},"outputId":"92092cf2-9913-40f7-f783-f4cb0b39cf05"},"source":["print(x.grad) # x.grad = dl / dx"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"d0djbxq7npX9","colab_type":"text"},"source":["You can also stop autograd from tracking history on Tensors\n","with ``.requires_grad=True`` either by wrapping the code block in\n","``with torch.no_grad():``\n","\n"]},{"cell_type":"code","metadata":{"id":"cJ5tmdf1npX9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"status":"ok","timestamp":1600488673548,"user_tz":240,"elapsed":546,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}},"outputId":"03d409b7-bace-4251-8ad4-6116e8aeb02d"},"source":["print(x.requires_grad)\n","t = x ** 2\n","print((x ** 2).requires_grad)\n","print(t.requires_grad)\n","\n","with torch.no_grad():\n","\tprint((x ** 2).requires_grad)\n","\tprint(t.requires_grad)\n","\t\n","DCG_node([x, t])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["True\n","True\n","True\n","False\n","True\n","0.data: tensor([-2.0547,  0.4526,  0.5030])\n","0.requires_grad: True\n","0.grad: tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])\n","0.grad_fn: None\n","0.is_leaf: True\n","\n","1.data: tensor([4.2218, 0.2048, 0.2530])\n","1.requires_grad: True\n","1.grad: None\n","1.grad_fn: <PowBackward0 object at 0x7f32bbd03320>\n","1.is_leaf: False\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  after removing the cwd from sys.path.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"tJWetN6BnpX_","colab_type":"text"},"source":["Or by using ``.detach()`` to get a new Tensor with the same\n","content but that does not require gradients:\n","\n"]},{"cell_type":"code","metadata":{"id":"EnjL4SOrnpYA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":101},"executionInfo":{"status":"ok","timestamp":1600488684943,"user_tz":240,"elapsed":811,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}},"outputId":"4e50b31e-da90-4d4c-977e-e4dc19dfbde5"},"source":["print(x.requires_grad)\n","y = x.detach()\n","print(y.requires_grad)\n","print(x.eq(y).all())\n","print(x)\n","print(y)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["True\n","False\n","tensor(True)\n","tensor([-2.0547,  0.4526,  0.5030], requires_grad=True)\n","tensor([-2.0547,  0.4526,  0.5030])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1h70lVZ7eTzZ","colab_type":"text"},"source":["## grad_fn"]},{"cell_type":"code","metadata":{"id":"gI8vesoOefTR","colab_type":"code","colab":{}},"source":["import torch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"90RNlzNseVcR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":84},"executionInfo":{"status":"ok","timestamp":1600488690378,"user_tz":240,"elapsed":415,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}},"outputId":"5ff0610c-5187-4a9b-e033-3ade50fa6f0c"},"source":["x1 = torch.tensor([1, 0.1, 0.09, 0.99], requires_grad=True)\n","x2 = x1 + 3\n","x3 = x2 ** 2\n","x4 = sum(x3)\n","print(x4.grad_fn)\n","print(x3.grad_fn)\n","print(x2.grad_fn)\n","print(x1.grad_fn)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<AddBackward0 object at 0x7f32bbd03f28>\n","<PowBackward0 object at 0x7f32bbd03f98>\n","<AddBackward0 object at 0x7f32bbd03f28>\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Z7-BbO-Vg-mL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600488697616,"user_tz":240,"elapsed":386,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}},"outputId":"8272033d-3bd2-4b37-b1e3-589bdea70276"},"source":["print(x4.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<AddBackward0 object at 0x7f32bbd03a90>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"91Pl1KApzRwC","colab_type":"text"},"source":["in the function\n","------------------------\n","create a function to test it"]},{"cell_type":"code","metadata":{"id":"EkZtKjDkzW4S","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":349},"executionInfo":{"status":"ok","timestamp":1600488714280,"user_tz":240,"elapsed":410,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}},"outputId":"18723912-6bc9-4835-ef26-81269c6a5b5f"},"source":["x = torch.tensor(torch.FloatTensor([[1, 2], [3, 4]]), requires_grad=True)\n","y = torch.sum(x ** 2)\n","y.backward()\n","DCG_node([x, y])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.data: tensor([[1., 2.],\n","        [3., 4.]])\n","0.requires_grad: True\n","0.grad: tensor([[2., 4.],\n","        [6., 8.]])\n","0.grad_fn: None\n","0.is_leaf: True\n","\n","1.data: 30.0\n","1.requires_grad: True\n","1.grad: None\n","1.grad_fn: <SumBackward0 object at 0x7f32bbd03eb8>\n","1.is_leaf: False\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \"\"\"Entry point for launching an IPython kernel.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  after removing the cwd from sys.path.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"VpL5gdTl0x8A","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":339},"executionInfo":{"status":"ok","timestamp":1600488735677,"user_tz":240,"elapsed":784,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}},"outputId":"2ec75f69-141d-4452-8bb4-51eb2a796011"},"source":["x = torch.tensor(torch.FloatTensor([[1, 2], [3, 4]]), requires_grad=True)\n","def sum_square(x):\n","    y = torch.sum(x ** 3)\n","    u = y * 2\n","    return u\n","z = sum_square(x)\n","z.backward()\n","DCG_node([x, z])\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.data: tensor([[1., 2.],\n","        [3., 4.]])\n","0.requires_grad: True\n","0.grad: tensor([[ 6., 24.],\n","        [54., 96.]])\n","0.grad_fn: None\n","0.is_leaf: True\n","\n","1.data: 200.0\n","1.requires_grad: True\n","1.grad: None\n","1.grad_fn: <MulBackward0 object at 0x7f32bbd51f98>\n","1.is_leaf: False\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \"\"\"Entry point for launching an IPython kernel.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  after removing the cwd from sys.path.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"ztrs-6OinpYC","colab_type":"text"},"source":["**Read Later:**\n","\n","Document about ``autograd.Function`` is at\n","https://pytorch.org/docs/stable/autograd.html#function\n","\n"]}]}