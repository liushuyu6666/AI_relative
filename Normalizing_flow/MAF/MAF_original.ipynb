{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MAF_original.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1vlC1eKXGw-c8uqgrYulFCkXyahxUOfKW","authorship_tag":"ABX9TyNiYZ8LXZlouKfxweFG9s9q"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jDM3RWEEoela"},"source":["packages\n","============="]},{"cell_type":"code","metadata":{"id":"JSBDmss6nL7_","executionInfo":{"status":"ok","timestamp":1606574002233,"user_tz":300,"elapsed":339,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.distributions as D\n","import torchvision.transforms as T\n","from torchvision.utils import save_image\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","import matplotlib\n","matplotlib.use('Agg')\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","\n","import os\n","import math\n","import argparse\n","import pprint\n","import copy\n","import gzip\n","import pickle"],"execution_count":39,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UJUenXCQogaJ"},"source":["parser\n","================"]},{"cell_type":"code","metadata":{"id":"ix03kuiTocFS","executionInfo":{"status":"ok","timestamp":1606575238276,"user_tz":300,"elapsed":401,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}}},"source":["parser = {'activation_fn': 'relu',\n"," 'batch_size': 100,\n"," 'cond_label_size': None,\n"," 'conditional': False,\n"," 'data_dir': './data/',\n"," 'dataset': 'MNIST',\n"," 'evaluate': False,\n"," 'flip_toy_var_order': False,\n"," 'generate': False,\n"," 'hidden_size': 100,\n"," 'input_dims': (1, 28, 28),\n"," 'input_order': 'sequential',\n"," 'input_size': 784,\n"," 'log_interval': 1000,\n"," 'lr': 0.0001,\n"," 'model': 'maf',\n"," 'n_blocks': 5,\n"," 'n_components': 1,\n"," 'n_epochs': 2,\n"," 'n_hidden': 1,\n"," 'no_batch_norm': False,\n"," 'no_cuda': False,\n"," 'output_dir': '/content/drive/MyDrive/Colab Notebooks/AI_relative/Normalizing_flow/result',\n"," 'restore_file': None,\n"," 'results_file': '/content/drive/MyDrive/Colab Notebooks/AI_relative/Normalizing_flow/result/results.txt',\n"," 'seed': 1,\n"," 'start_epoch': 0,\n"," 'train': True}"],"execution_count":66,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P3UvjxLiQ1zs"},"source":["one_hot_encode\n","===================="]},{"cell_type":"code","metadata":{"id":"lckM6dEbQ6zP","executionInfo":{"status":"ok","timestamp":1606573879897,"user_tz":300,"elapsed":494,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}}},"source":["def one_hot_encode(labels, n_labels):\n","    \"\"\"\n","    Transforms numeric labels to 1-hot encoded labels. Assumes numeric labels are in the range 0, 1, ..., n_labels-1.\n","    \"\"\"\n","\n","    assert np.min(labels) >= 0 and np.max(labels) < n_labels\n","\n","    y = np.zeros([labels.size, n_labels])\n","    y[range(labels.size), labels] = 1\n","\n","    return y"],"execution_count":34,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xvkolTMoQ7h_"},"source":["logit\n","===================="]},{"cell_type":"code","metadata":{"id":"ddYY3jbUQ8tK","executionInfo":{"status":"ok","timestamp":1606573877645,"user_tz":300,"elapsed":433,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}}},"source":["def logit(x):\n","    \"\"\"\n","    Elementwise logit (inverse logistic sigmoid).\n","    :param x: numpy array\n","    :return: numpy array\n","    \"\"\"\n","    return np.log(x / (1.0 - x))"],"execution_count":33,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MDhpR8IAROSc"},"source":["plot_hist_marginals\n","=============="]},{"cell_type":"code","metadata":{"id":"8Fn5z121ROla","executionInfo":{"status":"ok","timestamp":1606573960547,"user_tz":300,"elapsed":344,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}}},"source":["def plot_hist_marginals(data, lims=None, gt=None):\n","    \"\"\"\n","    Plots marginal histograms and pairwise scatter plots of a dataset.\n","    \"\"\"\n","\n","    n_bins = int(np.sqrt(data.shape[0]))\n","\n","    if data.ndim == 1:\n","\n","        fig, ax = plt.subplots(1, 1)\n","        ax.hist(data, n_bins, normed=True)\n","        ax.set_ylim([0, ax.get_ylim()[1]])\n","        if lims is not None: ax.set_xlim(lims)\n","        if gt is not None: ax.vlines(gt, 0, ax.get_ylim()[1], color='r')\n","\n","    else:\n","\n","        n_dim = data.shape[1]\n","        fig, ax = plt.subplots(n_dim, n_dim)\n","        ax = np.array([[ax]]) if n_dim == 1 else ax\n","\n","        if lims is not None:\n","            lims = np.asarray(lims)\n","            lims = np.tile(lims, [n_dim, 1]) if lims.ndim == 1 else lims\n","\n","        for i in xrange(n_dim):\n","            for j in xrange(n_dim):\n","\n","                if i == j:\n","                    ax[i, j].hist(data[:, i], n_bins, normed=True)\n","                    ax[i, j].set_ylim([0, ax[i, j].get_ylim()[1]])\n","                    if lims is not None: ax[i, j].set_xlim(lims[i])\n","                    if gt is not None: ax[i, j].vlines(gt[i], 0, ax[i, j].get_ylim()[1], color='r')\n","\n","                else:\n","                    ax[i, j].plot(data[:, i], data[:, j], 'k.', ms=2)\n","                    if lims is not None:\n","                        ax[i, j].set_xlim(lims[i])\n","                        ax[i, j].set_ylim(lims[j])\n","                    if gt is not None: ax[i, j].plot(gt[i], gt[j], 'r.', ms=8)\n","\n","    plt.show(block=False)\n","\n","    return fig, ax"],"execution_count":36,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4_404XvepDZ5"},"source":["create_masks\n","==================="]},{"cell_type":"code","metadata":{"id":"b26qhk6MpJzY","executionInfo":{"status":"ok","timestamp":1606572847493,"user_tz":300,"elapsed":508,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}}},"source":["def create_masks(input_size, hidden_size, n_hidden, input_order='sequential', input_degrees=None):\n","    # MADE paper sec 4:\n","    # degrees of connections between layers -- ensure at most in_degree - 1 connections\n","    degrees = []\n","\n","    # set input degrees to what is provided in args (the flipped order of the previous layer in a stack of mades);\n","    # else init input degrees based on strategy in input_order (sequential or random)\n","    if input_order == 'sequential':\n","        degrees += [torch.arange(input_size)] if input_degrees is None else [input_degrees]\n","        for _ in range(n_hidden + 1):\n","            degrees += [torch.arange(hidden_size) % (input_size - 1)]\n","        degrees += [torch.arange(input_size) % input_size - 1] if input_degrees is None else [input_degrees % input_size - 1]\n","\n","    elif input_order == 'random':\n","        degrees += [torch.randperm(input_size)] if input_degrees is None else [input_degrees]\n","        for _ in range(n_hidden + 1):\n","            min_prev_degree = min(degrees[-1].min().item(), input_size - 1)\n","            degrees += [torch.randint(min_prev_degree, input_size, (hidden_size,))]\n","        min_prev_degree = min(degrees[-1].min().item(), input_size - 1)\n","        degrees += [torch.randint(min_prev_degree, input_size, (input_size,)) - 1] if input_degrees is None else [input_degrees - 1]\n","\n","    # construct masks\n","    masks = []\n","    for (d0, d1) in zip(degrees[:-1], degrees[1:]):\n","        masks += [(d1.unsqueeze(-1) >= d0.unsqueeze(0)).float()]\n","\n","    return masks, degrees[0]"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7XebubCupPHp"},"source":["MaskedLinear\n","==================="]},{"cell_type":"code","metadata":{"id":"D4MQ-fS2pRnB","executionInfo":{"status":"ok","timestamp":1606572850352,"user_tz":300,"elapsed":771,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}}},"source":["class MaskedLinear(nn.Linear):\n","    \"\"\" MADE building block layer \"\"\"\n","    def __init__(self, input_size, n_outputs, mask, cond_label_size=None):\n","        super().__init__(input_size, n_outputs)\n","\n","        self.register_buffer('mask', mask)\n","\n","        self.cond_label_size = cond_label_size\n","        if cond_label_size is not None:\n","            self.cond_weight = nn.Parameter(torch.rand(n_outputs, cond_label_size) / math.sqrt(cond_label_size))\n","\n","    def forward(self, x, y=None):\n","   \n","        out = F.linear(x, self.weight * self.mask, self.bias)\n","        if y is not None:\n","            out = out + F.linear(y, self.cond_weight)\n","        return out\n","\n","    def extra_repr(self):\n","        return 'in_features={}, out_features={}, bias={}'.format(\n","            self.in_features, self.out_features, self.bias is not None\n","        ) + (self.cond_label_size != None) * ', cond_features={}'.format(self.cond_label_size)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZlHgfxOVpV1J"},"source":["BatchNorm\n","==============="]},{"cell_type":"code","metadata":{"id":"d8gQk3eNpWnS","executionInfo":{"status":"ok","timestamp":1606572854039,"user_tz":300,"elapsed":890,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}}},"source":["class BatchNorm(nn.Module):\n","    \"\"\" RealNVP BatchNorm layer \"\"\"\n","    def __init__(self, input_size, momentum=0.9, eps=1e-5):\n","        super().__init__()\n","        self.momentum = momentum\n","        self.eps = eps\n","\n","        self.log_gamma = nn.Parameter(torch.zeros(input_size))\n","        self.beta = nn.Parameter(torch.zeros(input_size))\n","\n","        self.register_buffer('running_mean', torch.zeros(input_size))\n","        self.register_buffer('running_var', torch.ones(input_size))\n","\n","    def forward(self, x, cond_y=None):\n","        if self.training:\n","            self.batch_mean = x.mean(0)\n","            self.batch_var = x.var(0) # note MAF paper uses biased variance estimate; ie x.var(0, unbiased=False)\n","\n","            # update running mean\n","            self.running_mean.mul_(self.momentum).add_(self.batch_mean.data * (1 - self.momentum))\n","            self.running_var.mul_(self.momentum).add_(self.batch_var.data * (1 - self.momentum))\n","\n","            mean = self.batch_mean\n","            var = self.batch_var\n","        else:\n","            mean = self.running_mean\n","            var = self.running_var\n","\n","        # compute normalized input (cf original batch norm paper algo 1)\n","        x_hat = (x - mean) / torch.sqrt(var + self.eps)\n","        y = self.log_gamma.exp() * x_hat + self.beta\n","\n","        # compute log_abs_det_jacobian (cf RealNVP paper)\n","        log_abs_det_jacobian = self.log_gamma - 0.5 * torch.log(var + self.eps)\n","#        print('in sum log var {:6.3f} ; out sum log var {:6.3f}; sum log det {:8.3f}; mean log_gamma {:5.3f}; mean beta {:5.3f}'.format(\n","#            (var + self.eps).log().sum().data.numpy(), y.var(0).log().sum().data.numpy(), log_abs_det_jacobian.mean(0).item(), self.log_gamma.mean(), self.beta.mean()))\n","        return y, log_abs_det_jacobian.expand_as(x)\n","\n","    def inverse(self, y, cond_y=None):\n","        if self.training:\n","            mean = self.batch_mean\n","            var = self.batch_var\n","        else:\n","            mean = self.running_mean\n","            var = self.running_var\n","\n","        x_hat = (y - self.beta) * torch.exp(-self.log_gamma)\n","        x = x_hat * torch.sqrt(var + self.eps) + mean\n","\n","        log_abs_det_jacobian = 0.5 * torch.log(var + self.eps) - self.log_gamma\n","\n","        return x, log_abs_det_jacobian.expand_as(x)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CPT7rY20pcKp"},"source":["FlowSequential\n","================="]},{"cell_type":"code","metadata":{"id":"r2ny5enIpezp","executionInfo":{"status":"ok","timestamp":1606572856496,"user_tz":300,"elapsed":514,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}}},"source":["class FlowSequential(nn.Sequential):\n","    \"\"\" Container for layers of a normalizing flow \"\"\"\n","    def forward(self, x, y):\n","        sum_log_abs_det_jacobians = 0\n","        for module in self:\n","            x, log_abs_det_jacobian = module(x, y)\n","            sum_log_abs_det_jacobians = sum_log_abs_det_jacobians + log_abs_det_jacobian\n","        return x, sum_log_abs_det_jacobians\n","\n","    def inverse(self, u, y):\n","        sum_log_abs_det_jacobians = 0\n","        for module in reversed(self):\n","            u, log_abs_det_jacobian = module.inverse(u, y)\n","            sum_log_abs_det_jacobians = sum_log_abs_det_jacobians + log_abs_det_jacobian\n","        return u, sum_log_abs_det_jacobians"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9H0cPIq_picE"},"source":["MADE\n","================"]},{"cell_type":"code","metadata":{"id":"tvP8Y2YopjV8","executionInfo":{"status":"ok","timestamp":1606572859960,"user_tz":300,"elapsed":310,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}}},"source":["class MADE(nn.Module):\n","    def __init__(self, input_size, hidden_size, n_hidden, cond_label_size=None, activation='relu', input_order='sequential', input_degrees=None):\n","        \"\"\"\n","        Args:\n","            input_size -- scalar; dim of inputs\n","            hidden_size -- scalar; dim of hidden layers\n","            n_hidden -- scalar; number of hidden layers\n","            activation -- str; activation function to use\n","            input_order -- str or tensor; variable order for creating the autoregressive masks (sequential|random)\n","                            or the order flipped from the previous layer in a stack of mades\n","            conditional -- bool; whether model is conditional\n","        \"\"\"\n","        super().__init__()\n","        # base distribution for calculation of log prob under the model\n","        self.register_buffer('base_dist_mean', torch.zeros(input_size))\n","        self.register_buffer('base_dist_var', torch.ones(input_size))\n","\n","        # create masks\n","        masks, self.input_degrees = create_masks(input_size, hidden_size, n_hidden, input_order, input_degrees)\n","\n","        # setup activation\n","        if activation == 'relu':\n","            activation_fn = nn.ReLU()\n","        elif activation == 'tanh':\n","            activation_fn = nn.Tanh()\n","        else:\n","            raise ValueError('Check activation function.')\n","\n","        # construct model\n","        self.net_input = MaskedLinear(input_size, hidden_size, masks[0], cond_label_size)\n","        self.net = []\n","        for m in masks[1:-1]:\n","            self.net += [activation_fn, MaskedLinear(hidden_size, hidden_size, m)]\n","        self.net += [activation_fn, MaskedLinear(hidden_size, 2 * input_size, masks[-1].repeat(2,1))]\n","        self.net = nn.Sequential(*self.net)\n","\n","    @property\n","    def base_dist(self):\n","        return D.Normal(self.base_dist_mean, self.base_dist_var)\n","\n","    def forward(self, x, y=None):\n","        # MAF eq 4 -- return mean and log std\n","        m, loga = self.net(self.net_input(x, y)).chunk(chunks=2, dim=1)\n","        u = (x - m) * torch.exp(-loga)\n","        # MAF eq 5\n","        log_abs_det_jacobian = - loga\n","        return u, log_abs_det_jacobian\n","\n","    def inverse(self, u, y=None, sum_log_abs_det_jacobians=None):\n","        # MAF eq 3\n","        D = u.shape[1]\n","        x = torch.zeros_like(u)\n","        # run through reverse model\n","        for i in self.input_degrees:\n","            m, loga = self.net(self.net_input(x, y)).chunk(chunks=2, dim=1)\n","            x[:,i] = u[:,i] * torch.exp(loga[:,i]) + m[:,i]\n","        log_abs_det_jacobian = loga\n","        return x, log_abs_det_jacobian\n","\n","    def log_prob(self, x, y=None):\n","        u, log_abs_det_jacobian = self.forward(x, y)\n","        return torch.sum(self.base_dist.log_prob(u) + log_abs_det_jacobian, dim=1)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nia-uvJGpp1r"},"source":["MAF\n","=============="]},{"cell_type":"code","metadata":{"id":"Uy-hHgdEpqmd","executionInfo":{"status":"ok","timestamp":1606572862892,"user_tz":300,"elapsed":347,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}}},"source":["class MAF(nn.Module):\n","    def __init__(self, n_blocks, input_size, hidden_size, n_hidden, cond_label_size=None, activation='relu', input_order='sequential', batch_norm=True):\n","        super().__init__()\n","        # base distribution for calculation of log prob under the model\n","        self.register_buffer('base_dist_mean', torch.zeros(input_size))\n","        self.register_buffer('base_dist_var', torch.ones(input_size))\n","\n","        # construct model\n","        modules = []\n","        self.input_degrees = None\n","        print(f\"input size is {input_size}\")\n","        print(input_size)\n","        for i in range(n_blocks):\n","            modules += [MADE(input_size, hidden_size, n_hidden, cond_label_size, activation, input_order, self.input_degrees)]\n","            self.input_degrees = modules[-1].input_degrees.flip(0)\n","            modules += batch_norm * [BatchNorm(input_size)]\n","\n","        self.net = FlowSequential(*modules)\n","\n","    @property\n","    def base_dist(self):\n","        return D.Normal(self.base_dist_mean, self.base_dist_var)\n","\n","    def forward(self, x, y=None):\n","        return self.net(x, y)\n","\n","    def inverse(self, u, y=None):\n","        return self.net.inverse(u, y)\n","\n","    def log_prob(self, x, y=None):\n","        u, sum_log_abs_det_jacobians = self.forward(x, y)\n","        return torch.sum(self.base_dist.log_prob(u) + sum_log_abs_det_jacobians, dim=1)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OTNBxdeSpxqp"},"source":["train\n","================"]},{"cell_type":"code","metadata":{"id":"hUFSferRpy4p","executionInfo":{"status":"ok","timestamp":1606575208094,"user_tz":300,"elapsed":291,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}}},"source":["def train(model, dataloader, optimizer, epoch):\n","\n","    for i, data in enumerate(dataloader):\n","        model.train()\n","\n","        # check if labeled dataset\n","        if len(data) == 1:\n","            x, y = data[0], None\n","        else:\n","            x, y = data\n","            # y = y.to(args.device)\n","        x = x.view(x.shape[0], -1)\n","        # x = x.view(x.shape[0], -1).to(args.device)\n","\n","        loss = - model.log_prob(x, y if parser['cond_label_size'] else None).mean(0)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if i % parser['log_interval'] == 0:\n","            print('epoch {:3d} / {}, step {:4d} / {}; loss {:.4f}'.format(\n","                epoch, parser['start_epoch'] + parser['n_epochs'], i, len(dataloader), loss.item()))"],"execution_count":64,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"At7-e-g4p4RS"},"source":["evalutate\n","====================="]},{"cell_type":"code","metadata":{"id":"jqfhdPVhp7H8","executionInfo":{"status":"ok","timestamp":1606575147166,"user_tz":300,"elapsed":285,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}}},"source":["@torch.no_grad()\n","def evaluate(model, dataloader, epoch):\n","    model.eval()\n","\n","    # conditional model\n","    if parser['cond_label_size'] is not None:\n","        logprior = torch.tensor(1 / parser['cond_label_size']).log()#.to(args.device)\n","        loglike = [[] for _ in range(parser['cond_label_size'])]\n","\n","        for i in range(parser['cond_label_size']):\n","            # make one-hot labels\n","            labels = torch.zeros(parser['batch_size'], parser['cond_label_size'])#.to(args.device)\n","            labels[:,i] = 1\n","\n","            for x, y in dataloader:\n","                x = x.view(x.shape[0], -1)#.to(args.device)\n","                loglike[i].append(model.log_prob(x, labels))\n","\n","            loglike[i] = torch.cat(loglike[i], dim=0)   # cat along data dim under this label\n","        loglike = torch.stack(loglike, dim=1)           # cat all data along label dim\n","\n","        # log p(x) = log ∑_y p(x,y) = log ∑_y p(x|y)p(y)\n","        # assume uniform prior      = log p(y) ∑_y p(x|y) = log p(y) + log ∑_y p(x|y)\n","        logprobs = logprior + loglike.logsumexp(dim=1)\n","        # TODO -- measure accuracy as argmax of the loglike\n","\n","    # unconditional model\n","    else:\n","        logprobs = []\n","        for data in dataloader:\n","            x = data[0].view(data[0].shape[0], -1)#.to(args.device)\n","            logprobs.append(model.log_prob(x))\n","        logprobs = torch.cat(logprobs, dim=0)#.to(args.device)\n","\n","    logprob_mean, logprob_std = logprobs.mean(0), 2 * logprobs.var(0).sqrt() / math.sqrt(len(dataloader.dataset))\n","    output = 'Evaluate ' + (epoch != None)*'(epoch {}) -- '.format(epoch) + 'logp(x) = {:.3f} +/- {:.3f}'.format(logprob_mean, logprob_std)\n","    print(output)\n","    print(output, file=open(parser['results_file'], 'a'))\n","    return logprob_mean, logprob_std"],"execution_count":60,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mhdjcf2uqBoU"},"source":["generate\n","================"]},{"cell_type":"code","metadata":{"id":"MkWacZksqCmx","executionInfo":{"status":"ok","timestamp":1606575144595,"user_tz":300,"elapsed":332,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}}},"source":["@torch.no_grad()\n","def generate(model, dataset_lam, step=None, n_row=10):\n","    model.eval()\n","\n","    # conditional model\n","    if parser['cond_label_size']:\n","        samples = []\n","        labels = torch.eye(parser['cond_label_size'])#.to(args.device)\n","\n","        for i in range(parser['cond_label_size']):\n","            # sample model base distribution and run through inverse model to sample data space\n","            u = model.base_dist.sample((n_row, parser['n_components'])).squeeze()\n","            labels_i = labels[i].expand(n_row, -1)\n","            sample, _ = model.inverse(u, labels_i)\n","            log_probs = model.log_prob(sample, labels_i).sort(0)[1].flip(0)  # sort by log_prob; take argsort idxs; flip high to low\n","            samples.append(sample[log_probs])\n","\n","        samples = torch.cat(samples, dim=0)\n","\n","    # unconditional model\n","    else:\n","        u = model.base_dist.sample((n_row**2, parser['n_components'])).squeeze()\n","        samples, _ = model.inverse(u)\n","        log_probs = model.log_prob(samples).sort(0)[1].flip(0)  # sort by log_prob; take argsort idxs; flip high to low\n","        samples = samples[log_probs]\n","\n","    # convert and save images\n","    samples = samples.view(samples.shape[0], *parser['input_dims'])\n","    samples = (torch.sigmoid(samples) - dataset_lam) / (1 - 2 * dataset_lam)\n","    filename = 'generated_samples' + (step != None)*'_epoch_{}'.format(step) + '.png'\n","    save_image(samples, os.path.join(parser['output_dir'], filename), nrow=n_row, normalize=True)"],"execution_count":59,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IYtZkKukqH9q"},"source":["train_and_evaluate\n","=========================="]},{"cell_type":"code","metadata":{"id":"rSXnreU5qJ7y","executionInfo":{"status":"ok","timestamp":1606575191791,"user_tz":300,"elapsed":315,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}}},"source":["def train_and_evaluate(model, train_loader, test_loader, optimizer):\n","    best_eval_logprob = float('-inf')\n","\n","    for i in range(parser['start_epoch'], parser['start_epoch'] + parser['n_epochs']):\n","        train(model, train_loader, optimizer, i)\n","        eval_logprob, _ = evaluate(model, test_loader, i)\n","\n","        # save training checkpoint\n","        torch.save({'epoch': i,\n","                    'model_state': model.state_dict(),\n","                    'optimizer_state': optimizer.state_dict()},\n","                    os.path.join(parser['output_dir'], 'model_checkpoint.pt'))\n","        # save model only\n","        torch.save(model.state_dict(), os.path.join(parser['output_dir'], 'model_state.pt'))\n","\n","        # save best state\n","        if eval_logprob > best_eval_logprob:\n","            best_eval_logprob = eval_logprob\n","            torch.save({'epoch': i,\n","                        'model_state': model.state_dict(),\n","                        'optimizer_state': optimizer.state_dict()},\n","                        os.path.join(parser['output_dir'], 'best_model_checkpoint.pt'))\n","\n","        # plot sample\n","        if parser['dataset'] == 'TOY':\n","            plot_sample_and_density(model, train_loader.dataset.base_dist, step=i)\n","        if parser['dataset'] == 'MNIST':\n","            generate(model, train_loader.dataset.lam, step=i)"],"execution_count":62,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"owSPSfzOO3Yl"},"source":["MNIST\n","========================"]},{"cell_type":"code","metadata":{"id":"1LGFslOTO4xA","executionInfo":{"status":"ok","timestamp":1606573975400,"user_tz":300,"elapsed":380,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}}},"source":["class MNIST:\n","    \"\"\"\n","    The MNIST dataset of handwritten digits.\n","    \"\"\"\n","\n","    alpha = 1.0e-6\n","\n","    class Data:\n","        \"\"\"\n","        Constructs the dataset.\n","        \"\"\"\n","\n","        def __init__(self, data, logit, dequantize, rng):\n","\n","            x = self._dequantize(data[0], rng) if dequantize else data[0]  # dequantize pixels\n","            self.x = self._logit_transform(x) if logit else x              # logit\n","            self.labels = data[1]                                          # numeric labels\n","            self.y = one_hot_encode(self.labels, 10)                  # 1-hot encoded labels\n","            self.N = self.x.shape[0]                                       # number of datapoints\n","\n","        @staticmethod\n","        def _dequantize(x, rng):\n","            \"\"\"\n","            Adds noise to pixels to dequantize them.\n","            \"\"\"\n","            return x + rng.rand(*x.shape) / 256.0\n","\n","        @staticmethod\n","        def _logit_transform(x):\n","            \"\"\"\n","            Transforms pixel values with logit to be unconstrained.\n","            \"\"\"\n","            return logit(MNIST.alpha + (1 - 2*MNIST.alpha) * x)\n","\n","    def __init__(self, logit=True, dequantize=True):\n","\n","        # load dataset\n","        f = gzip.open('/content/drive/MyDrive/Colab Notebooks/AI_relative/Normalizing_flow/data/mnist.pkl.gz', 'rb')\n","        trn, val, tst = pickle.load(f, encoding='latin1')\n","        f.close()\n","\n","        rng = np.random.RandomState(42)\n","        self.trn = self.Data(trn, logit, dequantize, rng)\n","        self.val = self.Data(val, logit, dequantize, rng)\n","        self.tst = self.Data(tst, logit, dequantize, rng)\n","        \n","        \n","        print(\"label shape\")\n","        print(self.trn.labels.shape, self.val.labels.shape, self.tst.labels.shape)\n","        \n","        print(\"N\")\n","        print(self.trn.N, self.val.N, self.tst.N)\n","        \n","\n","        im_dim = int(np.sqrt(self.trn.x.shape[1]))\n","        self.n_dims = (1, im_dim, im_dim)\n","        self.n_labels = self.trn.y.shape[1]\n","        self.image_size = [im_dim, im_dim]\n","\n","    def show_pixel_histograms(self, split, pixel=None):\n","        \"\"\"\n","        Shows the histogram of pixel values, or of a specific pixel if given.\n","        \"\"\"\n","\n","        data_split = getattr(self, split, None)\n","        if data_split is None:\n","            raise ValueError('Invalid data split')\n","\n","        if pixel is None:\n","            data = data_split.x.flatten()\n","\n","        else:\n","            row, col = pixel\n","            idx = row * self.image_size[0] + col\n","            data = data_split.x[:, idx]\n","\n","        n_bins = int(np.sqrt(data_split.N))\n","        fig, ax = plt.subplots(1, 1)\n","        ax.hist(data, n_bins, normed=True)\n","        plt.show()\n","\n","    def show_images(self, split):\n","        \"\"\"\n","        Displays the images in a given split.\n","        :param split: string\n","        \"\"\"\n","\n","        # get split\n","        data_split = getattr(self, split, None)\n","        if data_split is None:\n","            raise ValueError('Invalid data split')\n","\n","        # display images\n","        disp_imdata(data_split.x, self.image_size, [6, 10])\n","\n","        plt.show()\n"],"execution_count":37,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5lw6A5TxOA-Q"},"source":["fetch_dataloaders\n","=========================="]},{"cell_type":"code","metadata":{"id":"kXV_3rTsOD5Q","executionInfo":{"status":"ok","timestamp":1606574034089,"user_tz":300,"elapsed":297,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}}},"source":["def fetch_dataloaders(dataset_name, batch_size, flip_toy_var_order=False, toy_train_size=25000, toy_test_size=5000):\n","\n","    # grab datasets\n","    # if dataset_name in ['GAS', 'POWER', 'HEPMASS', 'MINIBOONE', 'BSDS300']:  # use the constructors by MAF authors\n","    #     dataset = load_dataset(dataset_name)()\n","\n","    #     # join train and val data again\n","    #     train_data = np.concatenate((dataset.trn.x, dataset.val.x), axis=0)\n","\n","    #     # construct datasets\n","    #     train_dataset = TensorDataset(torch.from_numpy(train_data.astype(np.float32)))\n","    #     test_dataset  = TensorDataset(torch.from_numpy(dataset.tst.x.astype(np.float32)))\n","\n","    #     input_dims = dataset.n_dims\n","    #     label_size = None\n","    #     lam = None\n","\n","    if dataset_name in ['MNIST']:\n","        dataset = MNIST()\n","\n","        # join train and val data again\n","        train_x = np.concatenate((dataset.trn.x, dataset.val.x), axis=0).astype(np.float32)\n","        train_y = np.concatenate((dataset.trn.y, dataset.val.y), axis=0).astype(np.float32)\n","\n","        # construct datasets\n","        train_dataset = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n","        test_dataset  = TensorDataset(torch.from_numpy(dataset.tst.x.astype(np.float32)),\n","                                      torch.from_numpy(dataset.tst.y.astype(np.float32)))\n","\n","        input_dims = dataset.n_dims\n","        label_size = 10\n","        lam = dataset.alpha\n","\n","    # elif dataset_name in ['TOY', 'MOONS']:  # use own constructors\n","    #     train_dataset = load_dataset(dataset_name)(toy_train_size, flip_toy_var_order)\n","    #     test_dataset = load_dataset(dataset_name)(toy_test_size, flip_toy_var_order)\n","\n","    #     input_dims = train_dataset.input_size\n","    #     label_size = train_dataset.label_size\n","    #     lam = None\n","\n","    # imaging dataset pulled from torchvision\n","    # elif dataset_name in ['CIFAR10']:\n","    #     label_size = 10\n","\n","    #     # MAF logit trainform parameter (cf. MAF paper 4.3\n","    #     lam = 1e-6 if dataset_name == 'mnist' else 5e-2\n","\n","    #     # MAF paper converts image data to logit space via transform described in section 4.3\n","    #     image_transforms = T.Compose([T.ToTensor(),\n","    #                                   T.Lambda(lambda x: x + torch.rand(*x.shape) / 256.),    # dequantize (cf MAF paper)\n","    #                                   T.Lambda(lambda x: logit(lam + (1 - 2 * lam) * x))])    # to logit space (cf MAF paper)\n","    #     target_transforms = T.Lambda(lambda x: partial(one_hot, label_size=label_size)(x))\n","\n","    #     train_dataset = load_dataset(dataset_name)(root=datasets.root, train=True, transform=image_transforms, target_transform=target_transforms)\n","    #     test_dataset =  load_dataset(dataset_name)(root=datasets.root, train=True, transform=image_transforms, target_transform=target_transforms)\n","\n","    #     input_dims = train_dataset[0][0].shape\n","\n","    else:\n","        raise ValueError('Unrecognized dataset.')\n","\n","\n","    # keep input dims, input size and label size\n","    train_dataset.input_dims = input_dims\n","    train_dataset.input_size = int(np.prod(input_dims))\n","    train_dataset.label_size = label_size\n","    train_dataset.lam = lam\n","\n","    test_dataset.input_dims = input_dims\n","    test_dataset.input_size = int(np.prod(input_dims))\n","    test_dataset.label_size = label_size\n","    test_dataset.lam = lam\n","\n","    # construct dataloaders\n","    # kwargs = {'num_workers': 1, 'pin_memory': True} if device.type is 'cuda' else {}\n","\n","    train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size, shuffle=False)\n","\n","    return train_loader, test_loader"],"execution_count":41,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iETm1VHJNLT6"},"source":["run\n","=================="]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l0qYA_fGNMFi","executionInfo":{"status":"ok","timestamp":1606574040871,"user_tz":300,"elapsed":3835,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}},"outputId":"4d08ac9c-3f09-472a-d16f-0eb752f824c1"},"source":["train_dataloader, test_dataloader \\\n","= fetch_dataloaders('MNIST', 100, False)"],"execution_count":42,"outputs":[{"output_type":"stream","text":["label shape\n","(50000,) (10000,) (10000,)\n","N\n","50000 10000 10000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w0EXCFCPRtJM","executionInfo":{"status":"ok","timestamp":1606574106408,"user_tz":300,"elapsed":475,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}}},"source":["input_size = train_dataloader.dataset.input_size\n","input_dims = train_dataloader.dataset.input_dims\n","cond_label_size = train_dataloader.dataset.label_size if parser['conditional'] else None"],"execution_count":43,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zEVTF3-cR5iY","executionInfo":{"status":"ok","timestamp":1606574232591,"user_tz":300,"elapsed":305,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}},"outputId":"cd530685-2ca6-4c88-8662-0b3f56f1d559"},"source":["model = MAF(parser['n_blocks'], parser['input_size'], parser['hidden_size'], parser['n_hidden'], parser['cond_label_size'],\n","                    parser['activation_fn'], parser['input_order'], batch_norm=not parser['no_batch_norm'])"],"execution_count":44,"outputs":[{"output_type":"stream","text":["input size is 784\n","784\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"u4TfLMfJSV2_"},"source":["model = model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rhBdU616ShVa","executionInfo":{"status":"ok","timestamp":1606574572824,"user_tz":300,"elapsed":740,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}}},"source":["optimizer = torch.optim.Adam(model.parameters(), lr=parser['lr'], weight_decay=1e-6)"],"execution_count":49,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GWtu9223Tpup","executionInfo":{"status":"ok","timestamp":1606575346354,"user_tz":300,"elapsed":104034,"user":{"displayName":"shuyu liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-UwUZB4YXwmGHBIgiFBJ8IFzBw2jqdBSvC_0=s64","userId":"00742976267113831789"}},"outputId":"fc3a7f45-aa02-46ff-dd77-cf149ea94a32"},"source":["train_and_evaluate(model, train_dataloader, test_dataloader, optimizer)"],"execution_count":67,"outputs":[{"output_type":"stream","text":["epoch   0 / 2, step    0 / 600; loss 1655.9421\n","Evaluate (epoch 0) -- logp(x) = -1655.909 +/- 2.538\n","epoch   1 / 2, step    0 / 600; loss 1627.9717\n","Evaluate (epoch 1) -- logp(x) = -1641.836 +/- 2.574\n"],"name":"stdout"}]}]}